Basic XOR test of the good/bad nn trainer
=========================================


Introduction
------------

The premise here is pretty simple. Instead of defining a set of
"correct" input/output pairs for training, we create a set of "opposite"
outputs and use "good/bad" to trigger learning. If performing an output
is "good" then the network is trained with the output it just performed.
If performing an output is "bad" the network is trained with the
opposite to the output.


In action
---------

Firstly, we need a neural network to calculate the XOR function

    >>> import libs.nn.backpropnn
    >>> nn = libs.nn.backpropnn.NN(2, 2, 1)

Now we need a trainer

    >>> import trainer
    >>> t = trainer.Trainer(nn, 10, ((0,), (1,)))

This neural network doesn't do XOR correctly, so lets train it, starting
with 0, 0 which is incorrect initially

    >>> input = (0, 0)
    >>> output = t.getoutput(input)
    >>> output
    (0,)

    >>> t.good(input)
    >>> t.train()

    >>> t.getoutput(input)
    (0,)

    >>> input = (0, 1)
    >>> output = t.getoutput(input)
    >>> output
    (0,)

    >>> t.bad(input)
    >>> t.train()

    >>> t.getoutput(input)
    (1,)

    >>> input = (1, 0)
    >>> output = t.getoutput(input)
    >>> output
    (0,)

    >>> t.bad(input)
    >>> t.train()

    >>> output = t.getoutput(input)
    >>> output
    (1,)

    >>> input = (1, 1)
    >>> output = t.getoutput(input)
    >>> output
    (1,)

    >>> t.bad(input)
    >>> t.train()

    >>> t.getoutput(input)
    (0,)

The trainer is now set up

    >>> t.memory
    deque([((0, 0), (0,)), ((0, 1), (1,)), ((1, 0), (1,)), ((1, 1), (0,))], maxlen=10)

Now we can test it all at once

    >>> t.getoutput((0, 0))
    (0,)

    >>> t.getoutput((0, 1))
    (1,)

    >>> t.getoutput((1, 0))
    (1,)

    >>> t.getoutput((1, 1))
    (0,)


