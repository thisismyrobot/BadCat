Basic XOR test of the good/bad nn trainer
=========================================


Introduction
------------

The premise here is pretty simple. Instead of defining a set of "correct"
input/output pairs for training, we simple let the trainer know if the result
is "good" or "bad", "bad" leading to re-training until the output is "good".

In action
---------

Firstly, we need a neural network to calculate the XOR function - 2 inputs of
range 0-1, 2 hidden neurons and 1 output neuron.

    >>> import neurolab
    >>> nn = neurolab.net.newff([[0, 1], [0, 1]],[2, 1])

To train the network the trainer needs to know how to communicate with it, so
we make an adapter for the neurolab network. The adapter needs to be properly
defined else it throws errors

    >>> import trainer

    >>> class AdaptNeurolabNN(trainer.LearningToolAdapter):
    ...     pass
    >>> ann = AdaptNeurolabNN(nn)
    Traceback (most recent call last):
    ...
    TypeError: Can't ... with abstract methods ci, co, learn, process

    >>> class AdaptNeurolabNN(trainer.LearningToolAdapter):
    ...
    ...     @property
    ...     def ci(self):
    ...         return self._lt.ci
    ...
    ...     @property
    ...     def co(self):
    ...         return self._lt.co
    ...
    ...     def learn(self, inputs, outputs):
    ...         self._lt.train(inputs, outputs, show=None, epochs=1000)
    ...
    ...     def process(self, input):
    ...         return self._lt.step(input)

Now we can wrap (adapt) the neural network

    >>> ann = AdaptNeurolabNN(nn)

Now we need a trainer, which complains if we don't pass it an adapted nn.

    >>> t = trainer.Trainer(nn, 10, ())
    Traceback (most recent call last):
    ...
    Exception: Learning tools must be wrapped in LearningToolAdapter instances
    >>> t = trainer.Trainer(ann, 10, ())

This neural network doesn't do XOR correctly, so lets train it. Firstly
we train on 0, 0 and show the memory as we go.

    >>> input = (0, 0)
    >>> _ = t.getoutput(input)

    >>> t._mutatemomentum
    0.75

    >>> i = 0
    >>> while t.bad() != (0,):
    ...     i += 1

    >>> t._mutatemomentum < 0.75
    True

    >>> t.good()

    >>> t._mutatemomentum
    0.75

    >>> t._memory
    deque([((0, 0), (0,))], maxlen=10)

    >>> t.getoutput()
    (0,)

Then on 0, 1

    >>> input = (0, 1)
    >>> _ = t.getoutput(input)

    >>> while t.bad() != (1,):
    ...     pass

    >>> t.good()
    >>> t._memory
    deque([((0, 0), (0,)), ((0, 1), (1,))], maxlen=10)

    >>> t.getoutput()
    (1,)

Then 1, 0

    >>> input = (1, 0)
    >>> _ = t.getoutput(input)

    >>> while t.bad() != (1,):
    ...     pass

    >>> t.good()
    >>> t.getoutput()
    (1,)

Then 1, 1

    >>> input = (1, 1)
    >>> _ = t.getoutput(input)

    >>> while t.bad() != (0,):
    ...     pass

    >>> t.good()
    >>> t._memory
    deque([((0, 0), (0,)), ((0, 1), (1,)), ((1, 0), (1,)), ((1, 1), (0,))], maxlen=10)

    >>> t.getoutput()
    (0,)

Now we can test it all at once

    >>> t.getoutput((0, 0))
    (0,)

    >>> t.getoutput((0, 1))
    (1,)

    >>> t.getoutput((1, 0))
    (1,)

    >>> t.getoutput((1, 1))
    (0,)
